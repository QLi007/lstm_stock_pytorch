{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "404f8ac4",
   "metadata": {},
   "source": [
    "\n",
    "# LSTM Stock Prediction - Standalone Version\n",
    "\n",
    "This notebook contains a complete implementation of LSTM stock prediction with sequential validation, without any external dependencies.\n",
    "\n",
    "## Features\n",
    "- Download stock data from Yahoo Finance\n",
    "- Prepare data with technical indicators\n",
    "- Train LSTM model with custom loss function\n",
    "- Sequential validation with walk-forward testing\n",
    "- Market regime analysis\n",
    "- Visualizations and performance metrics\n",
    "\n",
    "## Setup and Requirements\n",
    "First, let's install the required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages\n",
    "!pip install -q yfinance pandas matplotlib seaborn scikit-learn torch tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0408b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "print(\"LSTM Stock Prediction - Standalone Version\")\n",
    "print(\"==========================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8f0cd",
   "metadata": {},
   "source": [
    "\n",
    "## LSTM Model Definition\n",
    "\n",
    "Now, let's define the LSTM model and the custom loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1650854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the LSTM model\n",
    "class LSTMPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for stock prediction with enhanced architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.3, \n",
    "                 bidirectional=False, use_attention=True):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Size of hidden state\n",
    "            num_layers: Number of LSTM layers\n",
    "            dropout: Dropout probability\n",
    "            bidirectional: Whether to use bidirectional LSTM\n",
    "            use_attention: Whether to use attention mechanism\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Determine output size based on bidirectional\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention layer\n",
    "        if use_attention:\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(lstm_output_size, lstm_output_size // 2),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(lstm_output_size // 2, 1)\n",
    "            )\n",
    "        \n",
    "        # Fully connected layers with residual connections\n",
    "        self.fc1 = nn.Linear(lstm_output_size, hidden_size)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Final output layer with Tanh activation for position sizing (-1 to 1)\n",
    "        self.fc_out = nn.Linear(hidden_size // 2, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Batch normalization to improve training stability\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better convergence.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'lstm' in name:\n",
    "                    # LSTM weights - use orthogonal initialization\n",
    "                    nn.init.orthogonal_(param)\n",
    "                else:\n",
    "                    # FC layers - use Xavier initialization\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "    \n",
    "    def apply_attention(self, lstm_output):\n",
    "        \"\"\"Apply attention mechanism to LSTM output.\"\"\"\n",
    "        # Calculate attention weights\n",
    "        attn_weights = self.attention(lstm_output)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        context = torch.bmm(attn_weights.transpose(1, 2), lstm_output)\n",
    "        return context.squeeze(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # LSTM forward pass\n",
    "        batch_size = x.size(0)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            # Apply attention to focus on important timesteps\n",
    "            context = self.apply_attention(lstm_out)\n",
    "        else:\n",
    "            # Use last timestep\n",
    "            context = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers with residual connections and batch normalization\n",
    "        out = self.fc1(context)\n",
    "        if batch_size > 1:  # BatchNorm1d requires batch size > 1\n",
    "            out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        if batch_size > 1:\n",
    "            out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        # Final output with Tanh activation for position sizing (-1 to 1)\n",
    "        out = self.fc_out(out)\n",
    "        out = self.tanh(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c55508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define custom loss functions\n",
    "class KellyDrawdownLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Kelly Criterion and Drawdown optimization.\n",
    "    \n",
    "    Combines:\n",
    "    1. Kelly Criterion for position sizing\n",
    "    2. Drawdown penalty\n",
    "    3. Smoothness penalty for equity curve\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.5, max_leverage=2.0, dd_weight=1.0, smoothness_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Weight between Kelly position and return maximization\n",
    "        self.max_leverage = max_leverage\n",
    "        self.dd_weight = dd_weight\n",
    "        self.smoothness_weight = smoothness_weight\n",
    "        \n",
    "    def forward(self, preds, targets):\n",
    "        # Portfolio returns based on predicted positions and actual returns\n",
    "        portfolio_returns = preds * targets\n",
    "        \n",
    "        # Kelly criterion component\n",
    "        mu = torch.mean(portfolio_returns)\n",
    "        sigma = torch.std(portfolio_returns) + 1e-6\n",
    "        \n",
    "        # Optimal Kelly fraction\n",
    "        kelly_f = torch.clamp(mu / (sigma**2 + 1e-6), -self.max_leverage, self.max_leverage)\n",
    "        \n",
    "        # Position sizing loss (MSE to optimal Kelly fraction)\n",
    "        position_loss = torch.mean((preds - kelly_f)**2)\n",
    "        \n",
    "        # Return maximization (negative mean return)\n",
    "        return_loss = -mu\n",
    "        \n",
    "        # Drawdown calculation\n",
    "        cum_returns = torch.cumsum(portfolio_returns, dim=0)\n",
    "        running_max = torch.cummax(cum_returns, dim=0)[0]\n",
    "        drawdowns = running_max - cum_returns\n",
    "        max_drawdown = torch.max(drawdowns)\n",
    "        \n",
    "        # Smoothness calculation - penalize large swings in returns\n",
    "        if len(portfolio_returns) > 1:\n",
    "            return_changes = torch.diff(portfolio_returns, dim=0)\n",
    "            smoothness_loss = torch.std(return_changes)\n",
    "        else:\n",
    "            smoothness_loss = torch.tensor(0.0, device=preds.device)\n",
    "        \n",
    "        # Combine losses with weights\n",
    "        kelly_component = self.alpha * position_loss + (1 - self.alpha) * return_loss\n",
    "        drawdown_component = max_drawdown * self.dd_weight\n",
    "        smoothness_component = smoothness_loss * self.smoothness_weight\n",
    "        \n",
    "        total_loss = kelly_component + drawdown_component + smoothness_component\n",
    "        \n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331443b",
   "metadata": {},
   "source": [
    "\n",
    "## Data Processing Class\n",
    "\n",
    "Now, let's define the data processing class that will handle downloading and preparing the stock data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0dbf7",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training Class\n",
    "\n",
    "Next, let's define the class for training the LSTM model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57342c",
   "metadata": {},
   "source": [
    "\n",
    "## Example Usage\n",
    "\n",
    "Now, let's demonstrate the usage of the LSTM stock prediction framework with a complete example.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_stock_colab_standalone.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
