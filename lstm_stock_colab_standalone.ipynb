{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1d9c8e",
   "metadata": {},
   "source": [
    "\n",
    "# LSTM Stock Prediction - Standalone Version with GPU Support\n",
    "\n",
    "This notebook contains a complete implementation of LSTM stock prediction with sequential validation, optimized for GPU acceleration in Google Colab.\n",
    "\n",
    "## Features\n",
    "- GPU-accelerated training\n",
    "- Download stock data from Yahoo Finance\n",
    "- Prepare data with technical indicators\n",
    "- Train LSTM model with custom loss function\n",
    "- Sequential validation with walk-forward testing\n",
    "- Market regime analysis\n",
    "- Visualizations and performance metrics\n",
    "\n",
    "## GPU Setup in Colab\n",
    "1. Go to Runtime > Change runtime type\n",
    "2. Select \"GPU\" as Hardware accelerator\n",
    "3. Click \"Save\"\n",
    "\n",
    "## Setup and Requirements\n",
    "First, let's install the required packages and configure GPU support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages\n",
    "!pip install -q yfinance pandas matplotlib seaborn scikit-learn torch tqdm\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95257966",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# GPU Configuration and Memory Management\n",
    "def setup_gpu():\n",
    "    \"\"\"Configure GPU settings and memory management.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Get GPU properties\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # Convert to GB\n",
    "        \n",
    "        print(f\"Using GPU: {gpu_name}\")\n",
    "        print(f\"Total GPU Memory: {gpu_memory:.2f} GB\")\n",
    "        \n",
    "        # Set CUDA device\n",
    "        torch.cuda.set_device(0)\n",
    "        \n",
    "        # Enable cuDNN benchmarking for faster training\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        # Set memory allocator settings\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(\"No GPU available. Using CPU.\")\n",
    "        return False\n",
    "\n",
    "# Initialize GPU\n",
    "device = torch.device(\"cuda\" if setup_gpu() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "print(\"LSTM Stock Prediction - Standalone Version with GPU Support\")\n",
    "print(\"==========================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5442ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def manage_gpu_memory():\n",
    "    \"\"\"Monitor and manage GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Get current GPU memory usage\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        cached = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        \n",
    "        print(f\"GPU Memory Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"GPU Memory Cached: {cached:.2f} GB\")\n",
    "        \n",
    "        # Clear cache if memory usage is high\n",
    "        if allocated > 0.8:  # If more than 80% memory is used\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            print(\"Cleared GPU cache\")\n",
    "            \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move data to specified device (CPU/GPU).\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648f674",
   "metadata": {},
   "source": [
    "\n",
    "## LSTM Model Definition\n",
    "\n",
    "Now, let's define the LSTM model and the custom loss function with GPU optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce86304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual Block with batch normalization and activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Linear(in_channels, out_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Linear(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better convergence.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += self.shortcut(identity)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Define the LSTM model with residual blocks\n",
    "class LSTMPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model for stock prediction with enhanced architecture, GPU optimization,\n",
    "    and residual blocks for better training.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.3, \n",
    "                 bidirectional=False, use_attention=True):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Size of hidden state\n",
    "            num_layers: Number of LSTM layers\n",
    "            dropout: Dropout probability\n",
    "            bidirectional: Whether to use bidirectional LSTM\n",
    "            use_attention: Whether to use attention mechanism\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        # LSTM layer with GPU optimization\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Determine output size based on bidirectional\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention layer\n",
    "        if use_attention:\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(lstm_output_size, lstm_output_size // 2),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(lstm_output_size // 2, 1)\n",
    "            )\n",
    "        \n",
    "        # Residual blocks for feature processing\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(lstm_output_size, hidden_size, dropout=dropout),\n",
    "            ResidualBlock(hidden_size, hidden_size, dropout=dropout),\n",
    "            ResidualBlock(hidden_size, hidden_size // 2, dropout=dropout)\n",
    "        )\n",
    "        \n",
    "        # Final output layer with Tanh activation for position sizing (-1 to 1)\n",
    "        self.fc_out = nn.Linear(hidden_size // 2, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better convergence.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'lstm' in name:\n",
    "                    # LSTM weights - use orthogonal initialization\n",
    "                    nn.init.orthogonal_(param)\n",
    "                else:\n",
    "                    # FC layers - use Xavier initialization\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "    \n",
    "    def apply_attention(self, lstm_output):\n",
    "        \"\"\"Apply attention mechanism to LSTM output.\"\"\"\n",
    "        # Calculate attention weights\n",
    "        attn_weights = self.attention(lstm_output)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        context = torch.bmm(attn_weights.transpose(1, 2), lstm_output)\n",
    "        return context.squeeze(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # LSTM forward pass\n",
    "        batch_size = x.size(0)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            # Apply attention to focus on important timesteps\n",
    "            context = self.apply_attention(lstm_out)\n",
    "        else:\n",
    "            # Use last timestep\n",
    "            context = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Process through residual blocks\n",
    "        out = self.res_blocks(context)\n",
    "        \n",
    "        # Final output with Tanh activation for position sizing (-1 to 1)\n",
    "        out = self.fc_out(out)\n",
    "        out = self.tanh(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define custom loss functions\n",
    "class KellyDrawdownLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Kelly Criterion and Drawdown optimization with GPU support.\n",
    "    \n",
    "    Combines:\n",
    "    1. Kelly Criterion for position sizing\n",
    "    2. Drawdown penalty\n",
    "    3. Smoothness penalty for equity curve\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.5, max_leverage=2.0, dd_weight=1.0, smoothness_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Weight between Kelly position and return maximization\n",
    "        self.max_leverage = max_leverage\n",
    "        self.dd_weight = dd_weight\n",
    "        self.smoothness_weight = smoothness_weight\n",
    "        \n",
    "    def forward(self, preds, targets):\n",
    "        # Ensure inputs are on the same device\n",
    "        preds = preds.to(preds.device)\n",
    "        targets = targets.to(preds.device)\n",
    "        \n",
    "        # Portfolio returns based on predicted positions and actual returns\n",
    "        portfolio_returns = preds * targets\n",
    "        \n",
    "        # Kelly criterion component\n",
    "        mu = torch.mean(portfolio_returns)\n",
    "        sigma = torch.std(portfolio_returns) + 1e-6\n",
    "        \n",
    "        # Optimal Kelly fraction\n",
    "        kelly_f = torch.clamp(mu / (sigma**2 + 1e-6), -self.max_leverage, self.max_leverage)\n",
    "        \n",
    "        # Position sizing loss (MSE to optimal Kelly fraction)\n",
    "        position_loss = torch.mean((preds - kelly_f)**2)\n",
    "        \n",
    "        # Return maximization (negative mean return)\n",
    "        return_loss = -mu\n",
    "        \n",
    "        # Drawdown calculation\n",
    "        cum_returns = torch.cumsum(portfolio_returns, dim=0)\n",
    "        running_max = torch.cummax(cum_returns, dim=0)[0]\n",
    "        drawdowns = running_max - cum_returns\n",
    "        max_drawdown = torch.max(drawdowns)\n",
    "        \n",
    "        # Smoothness calculation - penalize large swings in returns\n",
    "        if len(portfolio_returns) > 1:\n",
    "            return_changes = torch.diff(portfolio_returns, dim=0)\n",
    "            smoothness_loss = torch.std(return_changes)\n",
    "        else:\n",
    "            smoothness_loss = torch.tensor(0.0, device=preds.device)\n",
    "        \n",
    "        # Combine losses with weights\n",
    "        kelly_component = self.alpha * position_loss + (1 - self.alpha) * return_loss\n",
    "        drawdown_component = max_drawdown * self.dd_weight\n",
    "        smoothness_component = smoothness_loss * self.smoothness_weight\n",
    "        \n",
    "        total_loss = kelly_component + drawdown_component + smoothness_component\n",
    "        \n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2413a158",
   "metadata": {},
   "source": [
    "\n",
    "## Data Processing Class\n",
    "\n",
    "Now, let's define the data processing class that will handle downloading and preparing the stock data with GPU support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845fe6e",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training Class\n",
    "\n",
    "Next, let's define the class for training the LSTM model with GPU optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb953d3",
   "metadata": {},
   "source": [
    "\n",
    "## Example Usage\n",
    "\n",
    "Now, let's demonstrate the usage of the LSTM stock prediction framework with GPU acceleration.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_stock_colab_standalone.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
